% Copyright 2021 Edoardo Riggio

% Licensed under the Apache License, Version 2.0 (the "License");
% you may not use this file except in compliance with the License.
% You may obtain a copy of the License at

% 	http://www.apache.org/licenses/LICENSE-2.0

% Unless required by applicable law or agreed to in writing, software
% distributed under the License is distributed on an "AS IS" BASIS,
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
% See the License for the specific language governing permissions and
% limitations under the License.

\documentclass{article}

\usepackage{hyperref, amsmath, graphicx, amssymb, csquotes}
\usepackage{fancyvrb,newverbs,xcolor}

\graphicspath{ {./assets/} }

\definecolor{cverbbg}{gray}{0.93}

\newenvironment{cverbatim}
 {\SaveVerbatim{cverb}}
 {\endSaveVerbatim
  \flushleft\fboxrule=0pt\fboxsep=.5em
  \colorbox{cverbbg}{\BUseVerbatim{cverb}}%
  \endflushleft
}

\newenvironment{lcverbatim}
 {\SaveVerbatim{cverb}}
 {\endSaveVerbatim
  \flushleft\fboxrule=0pt\fboxsep=.5em
  \colorbox{cverbbg}{%
    \makebox[\dimexpr\linewidth-2\fboxsep][l]{\BUseVerbatim{cverb}}%
  }
  \endflushleft
}

\newcommand{\ctexttt}[1]{\colorbox{cverbbg}{\texttt{#1}}}
\newverbcommand{\cverb}
  {\setbox\verbbox\hbox\bgroup}
  {\egroup\colorbox{cverbbg}{\box\verbbox}}

\begin{document}
\begin{titlepage}
    \begin{center}
        \vspace*{1cm}
        
        \Huge
        \textbf{Numerical Computing Cheatsheet}
        
        \vspace{0.5cm}
        \LARGE
        
        \vspace{.5cm}
        
        Edoardo Riggio
   		  \vspace{1.5cm}
       
        \vfill
        
        \today
        
        \vspace{.8cm}
          \Large
          Numerical Computing - SA. 2021 \\
        Computer Science\\
        Universit\`{a} della Svizzera Italiana, Lugano\\
        
    \end{center}
\end{titlepage}

\tableofcontents

\newpage

\section{PageRank Algorithm}
The PageRank algorithm is entirely determined by the link structure of the World Wide Web. A page will thus have a high rank if other pages with high ranks link to it.

\subsection{Random Surfer Model}
This algorithm is based on the \textbf{Random Surfer Model}. Here we imagine a user going from one page to the other by randomly choosing an outgoing link from one page to the next. This process is also called \textbf{exploitation}. \\ \\
The problem with exploitation is that it could lead to dead-ends -- i.e. a page with no outgoing links, or cycles around cliques of interconnected pages. For this reason, we sometimes choose a random page from the web to navigate to. This process is called \textbf{exploration}.

\subsection{Markov Chains}
The random walk generated by the combination of exploitation and exploration is known as a \textbf{Markov Chain}. A Markov Chain -- or Markov Process -- is a stochastic process. Differently from other stochastic processes, it has the property of being memory-less. This means that the probability of future states are not dependent upon the steps that led up to the present state.

\subsubsection{Markov Matrix}
Let $W$ be the set of webpages that can be reached by a Markov Chain of hyperlinks, $n$ the number of pages in $W$, and $G$ the $n \times n$ connectivity matrix of a portion of the Web. The matrix $G$ will be composed as follows:

\[ g_{ij} = \begin{cases} 1 & \text{if there is a hyperlink from $i$ to $j$} \\ 0 & \text{otherwise} \end{cases} \] \\
From matrix $G$ we can determine the in-degree and out-degree of a page $j$. This can be computed as follows:

\[ r_i = \sum_j g_{ij} \]
\[ c_j = \sum_i g_{ij} \] \\
Where $r_i$ is the in-degree and $c_j$ is the out-degree. Let now $p$ be the probability that the random walk follows a link -- i.e. performs exploitation. A typical value for $p$ is 0.85. Let $\delta$ be the probability that a particular random page is chosen, and $1-p$ is the probability that some random page is chosen -- i.e. perform exploration. Then $\delta$ will have the following formulation:

\[ \delta = \frac{1-p}{n} \] \\
Now let $A$ be a matrix that comes from scaling $G$ by its column sums. The elements of matrix $A$ will be:

\[ a_{ij} = \begin{cases} p \cdot \displaystyle\frac{g_{ij}}{c_j} + \delta & \text{if}~c_j \neq 0 \\ \displaystyle\frac{1}{n} & \text{if}~ c_j = 0 \end{cases} \] \\
Matrix $A$ is the transition probability matrix for the Markov Chain -- for this reason this matrix is also known as the Markov matrix. All of its elements are strictly between 1 and 0, and its column sums are all equal to 1.

\subsection{Eigenvectors and Eigenvalues}
\subsubsection{Eigenvector}
An \textbf{eigenvector} is a non-zero vector that changes at most by a scalar factor when a linear transformation is applied to it. 

\subsubsection{Eigenvalue}
An \textbf{eigenvalue} is the factor by which the eigenvector is scaled. Thus, this relation holds true:

\[ A\overrightarrow{v} = \lambda\overrightarrow{v} \] \\
Where $A$ is the transformation matrix, $\overrightarrow{v}$ is the eigenvector, and $\lambda$ is the eigenvector.

\subsubsection{Eigenbasis}
An \textbf{eigenbasis} is the basis vector space which consists entirely of eigenvectors. Mathematically, this is represented by a diagonal matrix in which all of its columns are eigenvectors. This basis is not always computable.

\subsubsection{Rayleigh Quotient}
The \textbf{Rayleigh Quotient} is a simple way to find -- given an eigenvector -- its corresponding eigenvalue. The following is the quotient:

\[ \mu(v) = \frac{v^T A v}{v^T v} \] \\
Where $v$ is the vector for which we need to compute the eigenvalue, and $A$ is the transformation matrix. One thing to note, is that if the vector $v$ is an eigenvector, then $\mu(v)$ will be the corresponding eigenvalue. On the other hand, if $v$ is not an eigenvector, then $\mu(v)$ will be the eigenvalue of the closest eigenvector of $v$. \\ \\
This quotient is useful in the computation of the dominant eigenvalue. This is done by the power iteration and by the inverse iteration.

\subsection{Power Iteration}
The \textbf{power iteration} is used in order to compute the dominant eigenvector $\lambda_1$ of a matrix $A$. In order to find it, we need to follow this algorithm:

\begin{enumerate}
	\item Start with an initial guess $v_0$, which is a vector
	\item Compute $w$, s.t. $w = Av_k$
	\item Set $v_{k+1} = ||w||$
	\item Find the eigenvalue of $v_{k+1}$ by using the Rayleigh quotient
	\item Repeat steps 2 to 4 until the difference between the previous and current eigenvectors is under a certain threshold. Increment $k$ by 1
\end{enumerate}

\subsection{Inverse Iteration}
The \textbf{inverse iteration} is very similar to the power method. In the case of the inverse iteration, we can use it to find a particular eigenvalue -- which is not necessarily the dominant one. \\ \\
This can be done by applying the power method to $(A-\alpha I)^{-1}$ instead of $A$. By doing so, the $\alpha$ constant will contribute in making the non-dominant eigenvector the dominant one. \\ \\
This algorithm converges much faster than the previous one. On the other hand, it is also more computationally expensive than the previous. In this case we will need to compute a system of equations for every step of the algorithm.

\[ (A-\alpha I)^{-1} \overrightarrow{v} = \frac{1}{\lambda - \alpha}\overrightarrow{v}~~ \iff ~~(A-\alpha I) x_{n+1} = x_n \]

\section{Social Networks}
In order to find a partition in a graph, we need to use the second smallest eigenvector -- i.e. eigenvector $v_2$. This eigenvector is also known as the \textbf{Fiedler eigenvector}, and plays a very important role in graph partitioning. This is because all of the indices corresponding to vector entries can be divided as follows:

\[ v_{2i} \in \begin{cases} \text{Set 1} & \text{if}~ v_{2i} > 0 \\ \text{Set 2} & \text{if}~ v_{2i} < 0 \end{cases} \] \\
The arising partition minimizes the number of edges between the two sets. The second smallest eigenvalue $\lambda_2$ is greater than 0 iff we are considering a connected graph. The magnitude of $\lambda_2$ gives us an indication on how well-connected the overall graph is.

\subsection{Matrix Decompositions and Permutations}
\subsubsection{Cholesky Decomposition}
By using the \textbf{Cholesky Decomposition}, any symmetric positive definite matrix can be decomposed into the product:

\[ A = L L^T \] \\
Where $L$ is a lower triangular matrix wit positive diagonal elements -- also known as the Cholesky factor.

\subsubsection{Reverse Cuthill McKee Ordering}
The \textbf{Reverse Cuthill McKee Ordering} is the permutation of a sparse matrix $A$ -- which has a symmetric sparsity pattern -- into a banded matrix with a small bandwidth. The resulting matrix will have its non-zero elements closer to the diagonal of the matrix. \\ \\
This permutation can be very useful when dealing with large sparse matrices. If we wanted to perform a Cholesky Decomposition on such matrix, this would result in a very high number of fill-ins. This would make the computation of the Cholseky factor very inefficient. But, if we first use the Reverse Cuthill McKee Ordering, we would drastically reduce the number of fill-ins. This is because the permutation would move all of the elements closer to the diagonal of the matrix.

\subsection{Centrality}
\subsubsection{Degree Centrality}
The \textbf{degree centrality} of a matrix $A$ is defined as the number of links incident upon a node. This means that it represents the number of vertices each node has.

\subsubsection{Eigenvector Centrality}
\textbf{Eigenvector centrality} is a measure of the influence of a node in a network. All relationships originating from high-scoring nodes contribute more to the score of the node than connections from low-scoring nodes. \\ \\
A high eigenvector score means that a node is connected to many nodes who themselves have high scores.

\section{Graph Partitioning}
The graph partitioning problem is defined in the form of a graph $G$, such that it is possible to partition $G$ into smaller components with specific properties. \\ \\
There are several partitioning algorithms, and they divide into two main categories:

\begin{itemize}
	\item \textbf{Local Approaches}
	\item \textbf{Global Approaches}
\end{itemize}

\subsection{Graph Laplacian Matrix}
The \textbf{Graph Laplacian Matrix} is the main tool for spectral partitioning. This is the matrix representation of a graph $G$. A graph Laplacian matrix is computed in two ways:

\[ L = D - A \]
\[ L = D - W \] \\
Where $D$ is the degree matrix, $A$ is the adjacency matrix, and $W$ is the weight matrix. In the first case, we consider an \textbf{undirected and unweighted} graph, while in the second case, we consider an \textbf{undirected and weighted} graph. \\ \\
Some properties of the graph Laplacian matrix $L(G)$ are:

\begin{itemize}
	\item It is symmetric, meaning that the eigenvalues of $L(G)$ are real and its eigenvectors are real and orthogonal
	\item The eigenvalues of $L(G)$ are non-negative
\end{itemize}

\subsubsection{Degree Matrix}
A \textbf{degree matrix} is a diagonal matrix which contains the degree of each vertex. In the case of an \textbf{unweighted} graph, we consider as the diagonal entries the number of nodes that node is connected to. For example:

\[ D = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 1 \end{bmatrix} \] \\
Means that the first node is connected to 1 other node, the second node is connected to 2 other nodes, and the third node is connected to 1 other node. \\ \\
In the case of a \textbf{weighted} graph, the non-zero values of the diagonal matrix represent the sum of all the weights of the connected edges. For example:

\[ D = \begin{bmatrix} 0.3 & 0 & 0 \\ 0 & 1.2 & 0 \\ 0 & 0 & 1.7 \end{bmatrix} \] \\
Means that the first node has $n$ connected edges with total weight of 0.3, the second node has $n$ connected edges with total weight of 1.2, and the third node has $n$ connected edges with total weight of 1.7.

\subsubsection{Adjacency Matrix}
An \textbf{adjacency matrix} is a square matrix used to represent the connections of an \textbf{unweigthed} graph. If the graph is \textbf{undirected}, then its entries will be:

\[ a_{ij} = \begin{cases} 1 & \text{if $i$ is connected to $j$ or $j$ is connected to $i$} \\ 0 & \text{otherwise} \end{cases} \]

\subsubsection{Weight Matrix}
A \textbf{weight matrix} is a square matrix used to represent the connections of a \textbf{weighted} graph. If the graph is \textbf{undirected}, then its entries will be:

\[ w_{ij} = \begin{cases} w & \text{if $i$ is connected to $j$ or $j$ is connected to $i$} \\ 0 & \text{otherwise} \end{cases} \]\\
Where $w$ is the weight of the edge of the graph.

\subsection{Local Approaches}
The major drawback of this approach is the arbitrary initial partitioning of the vertex set, which can affect the final solution quality. \\ \\
Two algorithms that have a local approach are \textbf{Kerighan-Lin algorithm} and the \textbf{Fiduccia-Mattheyses algorithms}. These where the first two effective 2-way cuts by local strategy.

\subsection{Global Approaches}
Global approaches rely on the properties of the entire graph, rather than on an arbitrary initial partition.

\subsubsection{Spectral Bisection}
This algorithm is one of the most common in graph partitioning. Here the partitions are derived from the graph Laplacian matrix. \\ \\
In order to perform a spectral bisection, we need to follow these steps:

\begin{enumerate}
	\item \textbf{Pre-Processing}
	\vspace{.2cm} \\
	Here we compute the graph Laplacian matrix.
	
	\item \textbf{Decomposition}
	\vspace{.2cm} \\
	Here we compute the second smallest eigenvalue and corresponding eigenvector -- i.e. Fiedler eigenvector.
	
	\item \textbf{Grouping}
	\vspace{.2cm} \\
	Here we divide the nodes based on the sign of the second eigenvector. More specifically:
	
	\[ v_i \in \begin{cases} G_1 & \text{if $x_{2i} < 0$} \\ G_2 & \text{if $x_{2i} \geq 0$} \end{cases} \] \\
	Where $v_i$ is the $i$-th vertex of the graph, $G_1$ is the first partition, $G_2$ is the second partition, and $x_{2i}$ is the $i$-th entry of the second smallest eigenvector.
\end{enumerate}
By doing so, we will obtain two roughly balanced partitions with minimum edgecut.

\subsubsection{Inertial Bisection}
This algorithm relies on the geometric layout of the graph. The main idea is to find an hyperplane that runs through the centre of mass of the points. In order to find the partitions, we follow this algorithm:

\begin{enumerate}
	\item \textbf{Pre-Processing}
	\vspace{.2cm} \\
	Here we compute the center of mass of the points
	
	\begin{equation*}
 		\begin{split}
    		\overline{x} = \frac{1}{n} \sum^n_{i = 1}x_i
  		\end{split}
		\quad\quad\quad\quad
  		\begin{split}
    		\overline{y} = \frac{1}{n} \sum^n_{i = 1}y_i
  		\end{split}
	\end{equation*} \\
	And matrix $M$
	
	\begin{align*}
		S_{xx} &= \sum^n_{i=1} \left( x_i - \overline{x} \right)^2 \\
		S_{yy} &= \sum^n_{i=1} \left( y_i - \overline{y} \right)^2 \\
		S_{xy} &= \sum^n_{i=1} \left( (x_i - \overline{x}) (y_i - \overline{y}) \right) \\
	\end{align*}
	\[ M = \begin{bmatrix} S_{xx} & S_{xy} \\ S_{xy} & S_{yy} \end{bmatrix}	\]
	
	\item \textbf{Decomposition}
	\vspace{.2cm} \\
	Here we compute the smallest eigenvalue and corresponding eigenvector of matrix $M$. This eigenvector is used in order to minimize the distance of the nodes to the line.
	
	\[ u^T M u \] \\
	Where $u$ is the smallest eigenvector.
	
	\item \textbf{Grouping}
	\vspace{.2cm} \\
	Here we partition nodes associated with geometric coordinates around a direction normal to the partitioning hyperplane.
\end{enumerate}

\subsection{Recursive Bisection}
The \textbf{recursive bisection} algorithm is highly dependent on the decisions made during the earlier stages of the partitioning process. Furthermore, it also suffers from the lack of global information. Thus, it may result in suboptimal partitions.

\subsection{K-Way Partitioning}
The graph $G$ is initially coarsened down to a small number of vertices. A \textbf{k-way partitioning} of this much smaller graph is computed. Then, this partitioning is projected bask towards the original finer graph by successfully refining the partitioning at each intermediate step. \\ \\
The main difference between this method and the previous one, is that here global information about the graph is stored.

\section{Graph Clustering}
The main reason as to why we use clustering is the need to extract information from a set of data. \\ \\
Given a set of data points and some notion of similarity between two pairs of nodes, the goal of clustering is to divide the data points into several groups such that points in the same group are similar and points in different groups are dissimilar to each other. 

\subsection{Similarity Graphs}
In order to build such similarity graphs, we can use one of the following methods:

\begin{itemize}
	\item \textbf{$\epsilon$-neighbourhood graph}
	\vspace{.2cm} \\
	Here we connect all of the points whose pairwise distances are smaller than a threshold value $\epsilon$.
	
	\item \textbf{$k$-nearest neighbour graph}
	\vspace{.2cm} \\
	Here we connect vertex $v_i$ to vertex $v_j$, if $v_j$ is among the k-nearest neighbours of $v_i$.
	
	\item \textbf{Fully connected graph}
	\vspace{.2cm} \\
	Here we connect all points with positive similarity with each other, and we weight all edges by $s_{ij}$ -- i.e. the similarity function.
\end{itemize}

\subsection{Clustering Algorithms}
\subsubsection{Spectral Clustering}
In order to perform \textbf{spectral clustering} on a graph, we need to follow this algorithm:

\begin{enumerate}
	\item Generate a similarity graph based on one of the three methods described earlier (usually we use the $\epsilon$-neighbourhood graph)
	\item Create the adjacency matrix from the similarity graph
	\item Create the graph Laplacian Matrix and implement spectral clustering
	\begin{enumerate}
		\item Compute the second smallest eigenvalue and it corresponding eigenvector
		\item Divide the nodes based on the sign of the entries of the second eigenvector
	\end{enumerate}
\end{enumerate}
The number of nodes inside of each cluster is very balanced.

\subsubsection{K-Means Clustering}
The \textbf{k-means clustering} algorithm makes use of centroids. \textbf{Centroids} are imaginary or real nodes in a graph which represent the center of a cluster. \\ \\
Clusters are formed by finding the distance of each node from each centroid. A node will be added to a cluster if the distance to its centroid is minimum with respect to the distance to the other centroids. \\ \\
With every step of the algorithm, the clusters are refined. The algorithm will end whenever there is no possible further refinement . This algorithm is not guaranteed to find the optimal solution.

\section{Image Deblurring}
Here we used the Conjugate Gradient in order to deblur an image given the exact blurred image and the original transformation matrix. \\ \\
Matrix $B$ represents the blurred image, vector $b$ is the vectorized form of matrix $B$, matrix $X$ represents the original grayscale image -- where every entry corresponds to a pixel, the vector $x$ is the vectorized form of matrix $X$, and matrix $A$ indicates the transformation matrix coming from the repeated application of the \textbf{image kernel} -- which is the bulr effect in this case. The sizes of the presented matrices and vectors are:

\begin{itemize}
	\item $A \rightarrow n^2 \times n^2$
	\item $B \rightarrow n^2 \times n^2$
	\item $X \rightarrow n \times n$
	\item $\overrightarrow{b} \rightarrow n^2$
	\item $\overrightarrow{x} \rightarrow n^2$
\end{itemize}
With all of these matrices and vectors defined, we can write the following system of equations:

\[ Ax = b \] \\
By solving this system, we can recover the original image, getting rid of the blur effect. The blurred effect in the image is given by a weighted average of the surrounding pixels to a pixel. These weights are defined by the kernel matrix $K$. From this matrix, we can obtain the matrix $A$ such that the non-zero elements of each row of $A$ correspond to the values of $K$. \\ \\
$A$ is a $d^2$ banded matrix -- where $d \ll n$. This means that $A$ is a sparse matrix.

\subsection{Conjugate Gradient Method}
The complexity of Gaussian Elimination for very large linear systems of equations is too high. For this reason we need to find other ways to solve these linear systems of equations. There are two types of methods that we can use:

\begin{itemize}
	\item \textbf{Direct Methods}
	\vspace{.2cm} \\
	Used to compute the exact solution in $n$ steps. This method has a very high memory consumption due to additional fill-ins.
	
	\item \textbf{Iterative Methods}
	\vspace{.2cm} \\
	This methods use an arbitrary initial starting point in order to compute an approximate arbitrary solution. For this method there is no need for additional memory and it will converge after a few iterations. On the other side, these methods are very often less robust and not as general as direct methods.
\end{itemize}

\subsubsection{Definitions}
The \textbf{error} in step $m$ is the deviation of the computed point from the exact solution. This can be written as:

\[ e^{(m)} = x-x^{(m)} \] \\
Where $x^{(m)}$ is the computed point and $x$ is the exact solution. This error is not known during the iterations -- otherwise we would know the solution. \\ \\
The \textbf{residual} provides us with a measure of the real error. This is computed as:

\[ r^{(m)} = b-Ax^{(m)} \] \\
Where $x^{(m)}$ is the computed point.

\subsubsection{Steepest Descent}
The \textbf{steepest descent} algorithm is a precursor to the Conjugate Gradient algorithm. Here we do the following operations:

\begin{enumerate}
	\item Start with a random initial guess
	\item Take the gradient -- which is the direction of the deepest descent
	\item Compute the minimum of the gradient
	\item Take the new gradient s.t. it is orthogonal to the previous gradient
	\item Repeat steps 2 to 4 until a certain convergence criterion is met
\end{enumerate}
This algorithm has a very low convergence rate, but it does not take optimal steps in order to find the next approximate solution.

\subsubsection{Conjugate Gradient}
The speed of convergence of the Conjugate Gradient is determined by the condition number $\kappa(A)$ of matrix $A$. The larger $\kappa(A)$, the slower the improvement. \\ \\
If matrix $A$ were not to be positive-definite, we can apply this trick in order to obtain a positive-definite matrix back:

\[ A^TAx = A^Tb \rightarrow \tilde{A}x = \tilde{b} \] \\
The premultiplication of $A$ with $A^T$ will result in the positive-definite matrix $\tilde{A}$. \\ \\
In order to carry out the actual method, we can use a slightly modified version of the steepest descent. The difference between the two methods is that in the case of the Conjugate Gradient Method, the two gradients need to be $A$-orthogonal. This means that:

\[ d^T_i A d_j = 0 \] \\
Where $d_i$ and $d_j$ are two vectors. By doing so, this would result in the new residual being orthogonal to the old residual. Furthermore, it would mean that every new step will never redo or undo optimizations of previous steps.

\subsection{Condition Number}
If a small modification in $b$ results in a big change in $x$, then the system is said to be \textbf{ill-conditioned}, and the condition number $\kappa(A)$ will be big. This condition number can be computed as:

\[ \kappa(A) = \frac{\sigma_{\max}}{\sigma_{\min}} \] \\
Where $\sigma$ indicates the singular values of $A$. For a real symmetric matrix, the singular values are equal to the absolute value of the eigenvalues.

\[ \sigma = |\lambda| \]

\subsection{Preconditioner}
A symmetric positive preconditioner $P$ is selected such that $P^{-1}$ approximates to $\tilde{A}^{-1}$.
\begin{align*}
	P^{-1}\tilde{A}x & = P^{-1}\tilde{b} \\
	(L^{-1}\tilde{A}L^{-1})(Lx) & = L^{-1}\tilde{b} \\
	\bar{\tilde{A}}\bar{x} & = \bar{\tilde{b}}
\end{align*}
Where

\[ \kappa(P^{-1}\tilde{A}) \ll \kappa(\tilde{A}) \]\\
This is done in order to both decrease the condition number and to decrease the range of the eigenvalues.
The computation of the preconditioner should be relatively inexpensive to find. This is why we use the \textbf{Incomplete Cholesky factorization} in order to compute the preconditioner. By using this method, we only compute the Cholesky factors for the non-zero elements of $\tilde{A}$. This will give us the preconditioner:

\[ P = F^TF \] \\
Where $F$ is the sparse IC factor. This routine can fail since the existence of $F$ is not guaranteed. In order to amend to this issue, we can apply a diagonal shift to $P$. By doing so, we enforce positive-definiteness, thus making $F$ computable.

\section{Linear Programming}
\textbf{Linear programming} is an optimization method which aims to maximize or minimize a linear objective function subject to linear equality or inequality constraints. Such an example is:

\begin{align*}
	\max~~ & \sum_{i = 1}^n c_ix_i \\
	\text{s.t.}~~ & \sum_{j = 1}^n a_{1,j}x_j \leq h_1 \\
	~~&~~ \vdots \\
	~~& \sum_{j = 1}^n a_{m,j}x_j \leq h_m
\end{align*}
In this case we have the presence of linear constrains. This means that the values of the vector $x_i$ cannot assume any arbitrary value. The following needs to be respected:

\begin{itemize}
	\item Satisfy a set of relationships
	\item Satisfy the non-negativity condition
	\begin{itemize}
		\item Only positive values are accepted
		\item Values need to belong to the feasible region
	\end{itemize}
\end{itemize}
The previous linear problem can be re-written by using the following simplified notation:
\begin{equation*}
  \begin{split}
    \max~~ & z = c^Tx \\
	\text{s.t.}~~ & Ax \leq h \\
	~~& x \geq 0
  \end{split}
\quad\quad\quad\quad
  \begin{split}
    \min~~ & z = c^Tx \\
	\text{s.t.}~~ & Ax \geq h \\
	~~& x \geq 0
  \end{split}
\end{equation*} \\
Where $z = f(x)$ represents the value of the objective function, $c$ is a vector of coefficients, $x$ is a vector of unknowns, $A$ is the coefficient matrix, and $h$ is the vector coefficients of the constraints. The sizes of these matrices are vectors are:

\begin{itemize}
	\item $A \rightarrow m \times n$
	\item $\overrightarrow{c} \rightarrow n$
	\item $\overrightarrow{x} \rightarrow n$
	\item $\overrightarrow{h} \rightarrow m$
\end{itemize}
This is known as the \textbf{standard form} of a linear program. After writing the problem in a standard form, we need to graphically plot the feasible region which satisfies all of the constraints.

\subsection{Fundamental Theorem of Linear Programming}
In order to find the optimal value, we can make use of the \textbf{Fundamental Theorem of Linear Programming}.

\blockquote{
If a linear programming problem has a solution, it must occur at a vertex of the set of feasible solutions. If the problem has more than one solution, then at least one of them must occur at a vertex of the set of feasible solutions. In either case, the value of the objective function is unique.
}

\subsection{Simplex Method}
The \textbf{simplex method} is an algorithm used for solving linear programming problems that have two or more variables. It has an exponential worst-case complexity. \\ \\
In order to solve a linear programming problem with the simplex method, we need to do the following:

\begin{enumerate}
	\item Write the problem in standard form
	\item Transform inequalities into equalities by adding:
	\begin{enumerate}
		\item \textbf{Slack Variables} -- for maximization
		\item \textbf{Surplus Variables} -- for minimization
	\end{enumerate}
	\item Apply the iterative rule by exchanging the basic and nonbasic variable
	\item Repeat step 3 until the optimality criterion is not met
\end{enumerate}
By introducing \textbf{slack} and \textbf{surplus} variables to the standard form of the linear programming problem we could obtain:

\begin{equation*}
  \begin{split}
  	\max~~ & z = 3x+2y \\
	\text{s.t.}~~ & x+2y+s_1 = 4 \\
	~~& x-y+s_2 = 1 \\
	~~& x,y \geq 0;~ s_1, s_2 \geq 0
  \end{split}
\quad\quad\quad\quad
  \begin{split}
    \min~~ & z = 3x+2y \\
	\text{s.t.}~~ & x+2y-s_1 = 4 \\
	~~& x-y-s_2 = 1 \\
	~~& x,y \geq 0;~ s_1, s_2 \geq 0
  \end{split}
\end{equation*}

\subsubsection{Basic and Nonbasic Variables}
The following statements are true:

\begin{itemize}
	\item We can freely swap the rows of $A$, as long as we also swap the elements of vector $h$
	\item We can freely swap the columns of $A$, as long as we also swap the elements of vector $x$
\end{itemize}
Matrix $A$ can be split into two sub matrices:

\[ A = \begin{bmatrix} B & D \end{bmatrix} \] \\
Where $B$ is the matrix formed by $m$ linearly independent columns of $A$, and $D$ is the matrix containing the remaining columns. In the same way, we can split $x$ and $c$:

\[ x = \begin{bmatrix} x_B \\ x_D \end{bmatrix} ~~~~ c = \begin{bmatrix} c_B \\ c_D \end{bmatrix} \] \\
Now, we can rewrite the system $Ax = h$ to:

\begin{align*}
	Bx_B + Dx_D &= h \\
	Bx-B &= h - Dx_D \\
	x_B &= B^{-1}h - B^{-1}Dx_D
\end{align*} \\
Where $x_B$ are the \textbf{basic variables}, and $x_D$ are the \textbf{nonbasic variables}. This equation represents a general solution of the linear programming problem.

\subsubsection{Basic Solution}
If we set $x_D = 0$ in the above equation, we obtain:

\[ x_B = B^{-1}h \] \\
If the non-negativity condition for $x_B$ is satisfied, then this \textbf{basic solution} is feasible. In the case that one or more components of $x_B$ have value 0, then the solution is said to be \textbf{degenerate}. \\ \\
The basic solution corresponds to the vertices of the polytope -- which represents the feasible region of the linear programming problem.

\subsubsection{Optimal Basic Solution}
The existence of a feasible solution implies also the existence of an optimal basic solution. Moreover, the existence of an optimal solution implies the existence of an optimal basic solution. \\ \\
A downside is the fact that the number of possible basic solutions grows exponentially with the number of unknowns and constraints. The maximum possible number of iterations of the simplex method is:

\[ N = \frac{(m + n)!}{m!n!} \]

\subsection{Optimality Condition}
The \textbf{optimality condition} that needs to be checked at every iteration of the simplex method is based on the reduced cost coefficients.

\[ r_D = c^T_D - c^T_B B^{-1}D \] \\
Where $c_B$ represents the basic coefficients of the objective function, $c_D$ represents the nonbasic coefficients of the objective function, $B$ is the basic matrix, and $D$ is the nonbasic matrix. The optimality conditions are given by $r_D \leq 0$ for a \textbf{maximization} problem, and $r_D \geq 0$ for a \textbf{minimization} problem.

\subsection{Iterative Rule}
If the stopping criterion is not met, we select the highest reduced cost coefficient (in the case of maximization), or the lowest reduced cost coefficient (in the case of minimization). This needs to be brought inside of the basis. \\ \\
For all of the columns of the entering variable, we need to compute the following ratio:

\[ \frac{B^{-1}h}{B^{-1}D} \] \\
Where $B^{-1}$ represents the inverse of the basic matrix, $h$ is the vector coefficients of the constraints, and $D$ represents the non-basic matrix. Among all of the obtained ratios, we need to select the one that has the smallest positive value. This will be the departing variable.

\subsection{Auxiliary Minimization Problem}
By solving an initial \textbf{auxiliary minimization problem}, we find a feasible initial basic solution. It can be defined as follows -- by introducing artificial variables $u_1, ..., u_m$:

\begin{align*}
	\max~~ & z_{\text{aux}} = \sum_{i = 1}^n u_i \\
	\text{s.t.}~~ & \sum_{j = 1}^n a_{1,j}x_j+s_1+u_1 = h_1 \\
	~~&~~ \vdots \\
	~~& \sum_{j = 1}^n a_{m,j}x_j+s_m+u_m = h_m \\
	~~& x_i, ..., x_n \geq 0;~ s_1, ..., s_m \geq 0;~ u_1, ..., u_m \geq 0
\end{align*} \\
The auxiliary problem aims at minimizing the sum of the artificial variables. The optimal solution of the auxiliary problem would be achieved when all of the artificial variables have a value of 0. \\ \\
If $z_{\text{aux}}$ does not reach the value 0, then both the auxiliary problem and the original linear programming problem do not admit a feasible solution.

\end{document}






























 
