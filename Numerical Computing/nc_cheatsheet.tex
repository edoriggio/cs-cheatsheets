% Copyright 2021 Edoardo Riggio

% Licensed under the Apache License, Version 2.0 (the "License");
% you may not use this file except in compliance with the License.
% You may obtain a copy of the License at

% 	http://www.apache.org/licenses/LICENSE-2.0

% Unless required by applicable law or agreed to in writing, software
% distributed under the License is distributed on an "AS IS" BASIS,
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
% See the License for the specific language governing permissions and
% limitations under the License.

\documentclass{article}

\usepackage{hyperref, amsmath, graphicx, amssymb, csquotes}
\usepackage{fancyvrb,newverbs,xcolor}

\graphicspath{ {./assets/} }

\definecolor{cverbbg}{gray}{0.93}

\newenvironment{cverbatim}
 {\SaveVerbatim{cverb}}
 {\endSaveVerbatim
  \flushleft\fboxrule=0pt\fboxsep=.5em
  \colorbox{cverbbg}{\BUseVerbatim{cverb}}%
  \endflushleft
}

\newenvironment{lcverbatim}
 {\SaveVerbatim{cverb}}
 {\endSaveVerbatim
  \flushleft\fboxrule=0pt\fboxsep=.5em
  \colorbox{cverbbg}{%
    \makebox[\dimexpr\linewidth-2\fboxsep][l]{\BUseVerbatim{cverb}}%
  }
  \endflushleft
}

\newcommand{\ctexttt}[1]{\colorbox{cverbbg}{\texttt{#1}}}
\newverbcommand{\cverb}
  {\setbox\verbbox\hbox\bgroup}
  {\egroup\colorbox{cverbbg}{\box\verbbox}}

\begin{document}
\begin{titlepage}
    \begin{center}
        \vspace*{1cm}
        
        \Huge
        \textbf{Numerical Computing Cheatsheet}
        
        \vspace{0.5cm}
        \LARGE
        
        \vspace{.5cm}
        
        Edoardo Riggio
   		  \vspace{1.5cm}
       
        \vfill
        
        \today
        
        \vspace{.8cm}
          \Large
          Numerical Computing - SA. 2021 \\
        Computer Science\\
        Universit\`{a} della Svizzera Italiana, Lugano\\
        
    \end{center}
\end{titlepage}

\tableofcontents

\newpage

\section{PageRank Algorithm}
The PageRank algorithm is entirely determined by the link structure of the World Wide Web. A page will thus have a high rank if other pages with high ranks link to it.

\subsection{Random Surfer Model}
This algorithm is based on the \textbf{Random Surfer Model}. Here we imagine a user going from one page to the other by randomly choosing an outgoing link from one page to the next. This process is also called \textbf{exploitation}. The problem with exploitation is that it could lead to dead-ends -- i.e. a page with no outgoing links, or cycles around cliques of interconnected pages. For this reason, we sometimes choose a random page from the web to navigate to. This process is called \textbf{exploration}.

\subsection{Markov Chains}
The random walk generated by the combination of exploitation and exploration is known as a \textbf{Markov Chain}. A Markov Chain -- or Markov Process -- is a stochastic process. Differently from other stochastic processes, it has the property of being memory-less. This means that the probability of future states are not dependent upon the steps that led up to the present state. \\ \\
Let $W$ be the set of webpages that can be reached by a Markov Chain of hyperlinks, $n$ the number of pages in $W$, ad $G$ the $n \times n$ connectivity matrix of a portion of the Web. The matrix $G$ will be composed as follows:

\begin{align*}
	g_{ij} = \begin{cases} 1 & \text{if there is a hyperlink from $i$ to $j$} \\ 0 & \text{otherwise} \end{cases}
\end{align*}
From matrix $G$ we can determine the in-degree and out-degree of a page $j$. This can be computed as follows:

\[ r_i = \sum_j g_{ij} \]
\[ c_J = \sum_i g_{ij} \] \\
Where $r_i$ is the in-degree and $c_j$ is the out-degree. Let now $p$ be the probability that the random walk follows a link -- i.e. performs exploitation. A typical value for $p$ is 0.85. Let $\delta$ be the probability that a particular random page is chosen, and $1-p$ is the probability that some random page is chosen -- i.e. perform exploration. Then $\delta$ will have the following formulation:

\[ \delta = \frac{1-p}{n} \] \\
Now let $A$ be a matrix that comes from scaling $G$ by its column sums. The elements of matrix $A$ will be:

\begin{align*}
	a_{ij} = \begin{cases} \rho \cdot \frac{g_{ij}}{c_j} + \delta & \text{if}~c_j \neq 0 \\ \frac{1}{n} & \text{if}~ c_j = 0 \end{cases}
\end{align*}
Matrix $A$ is the transition probability matrix for the Markov Chain -- for this reason this matrix is also known as the Markov matrix. All of its elements are strictly between 1 and 0, and its column sums are all equal to 1.

\subsection{Power Method}
The \textbf{power method} is an algorithm used in order to produce the dominant eigenvector of matrix $A$. In order to do so, we need to repeat the following computation:

\[ x = G \cdot (x + e) \cdot (z + x) \] \\
Until $x$ settles down to several decimal places.

\subsection{Inverse Iteration}
The \textbf{inverse iteration} is an algorithm which has the same goal of the power method. In this case, we need to find the dominant eigenvector of $(A - \alpha)^{-1}$ rather than of $A$. By doing so, the convergence is faster, but the computations are more expensive -- this method solves a system of equations, while the power method simply multiplies matrices together.

\section{Spectral Graph Partitioning}
In order to find a partition in a graph, we need to use the second smallest eigenvector -- i.e. eigenvector $v_2$. This eigenvector is also known as the \textbf{Fiedler eigenvector}, and plays a very important role in graph partitioning. This is because all of the indices corresponding to vector entries can be divided as follows:

\begin{align*}
	v_{2i} \in \begin{cases} \text{Set 1} & \text{if}~ v_{2i} > 0 \\ \text{Set 2} & \text{if}~ v_{2i} < 0 \end{cases}
\end{align*}
The arising partition minimizes the number of edges between the two sets. The second smallest eigenvalue $\lambda_2$ is greater than 0 iff we are considering a connected graph. The magnitude of $\lambda_2$ gives us an indication on how well-connected the overall graph is.

\subsection{Laplacian Matrix}
The \textbf{Laplacian Matrix} is the matrix representation of a graph. It is computed by doing the following:

\[ L = D - A \] \\
Where $A$ is the adjacency matrix of the graph $G$, and $D$ is the degree matrix of $A$. The \textbf{degree matrix} is a diagonal matrix which contains information about the degree of each vertex -- i.e. to how many other nodes is a node connected to. \\ \\
From the Laplacian Matrix, we can compute the eigenvector $w_2$ and the respective eigenvalue $\lambda_2$.

\subsection{Degree Centrality}
The \textbf{degree centrality} is defined as the number of links incident upon a node. In other words, it represents the number of vertices a node has.

\subsection{Reverse Cuthill McKee Ordering and Cholesky Factor}
The \textbf{Reverse Cuthill McKee Ordering} is an algorithm used to permute a sparse matrix that has a symmetric sparsity pattern, into a band matrix form with a small bandwidth. In other words, the non-zero elements of the matrix get nearer to the diagonal of the matrix. \\ \\
The \textbf{Cholesky Factorization} is the decomposition of a positive-definite matrix into the product of a lower-triangular matrix and its conjugate transpose.

\section{Graph Partitioning}
\subsection{Spectral Partitioning}
The most common example of a global approach to graph partitioning is \textbf{spectral partitioning}. This is carried out through a method known as spectral bisection. The spectral method utilizes the spectral graph theorem of linear algebra. This enables the decomposition of a real symmetric matrix into eigenvalues, within an orthonormal basis of eigenvectors. \\ \\
A spectral bisection is computed using the second eigenvector corresponding with the second eigenvalue of the graph Laplacian Matrix. By thresholding the values of $w_2$ around 0, it will result in two roughly balanced partitions with minimum edgecut. On the other hand, by thresholding the values around the median of $w_2$, it will result in two strictly balanced partitions.

\subsection{Inertial Bisection}
Another common approach is \textbf{inertial bisection}. In this case the nodes of the graph are associated with a coordinate list. it thus relies on the geometric layout of the graph. \\ \\
The main goal of this method is to find a hyperplane running through the centre of gravity of the points. Such a line is chosen in a way that the sum of squares of the distances of the nodes to the line is minimized.

\subsection{Recursive Partitioning}
\textbf{Recursive bisection} is a technique which is highly dependent on the decisions made during the earlier stages of the partitioning process. Furthermore, it lacks of global information. \\ \\
On the other hand, \textbf{K-way partitioning} starts with the partitioning of a small set of vertices. After this, the obtained partition is projected back towards the original set of nodes in order to refine it. \\ \\
The main difference between these two recursive approaches is that k-way partitioning stores global information about the graph -- i.e. it takes into consideration the whole graph when creating the partitions, while recursive partitioning does not.

\section{Graph Clustering}
The main reason as to why we use clustering is the need to extract information from a set of data. This is done by grouping elements that are similar based on some metric. The main goal of clustering is to put together elements which display similar behaviours, while separating them from the others.

\subsection{Similarity Graphs}
Given a set of data points and some notion of similarity between two pairs of nodes, the goal of clustering is to divide the data points into several groups such that points in the same group are similar and points in different groups are dissimilar to each other. \\ \\
In order to build such similarity graphs, we can use one of the following methods:

\begin{itemize}
	\item \textbf{$\epsilon$-neighbourhood graph}
	\vspace{.2cm} \\
	Here we connect all of the points whose pairwise distances are smaller than a threshold value $\epsilon$.
	
	\item \textbf{$k$-nearest neighbour graph}
	\vspace{.2cm} \\
	Here we connect vertex $v_i$ to vertex $v_j$, if $v_j$ is among the k-nearest neighbours of $v_i$.
	
	\item \textbf{Fully connected graph}
	\vspace{.2cm} \\
	Here we connect all points with positive similarity with each other, and we weight all edges by $s_{ij}$ -- i.e. the similarity function.
\end{itemize}

\subsection{Spectral Clustering}
The \textbf{spectral clustering} algorithm creates clusters that have the same number of nodes inside them, without considering the spacial characteristics of each node.

\subsection{K-Means Clustering}
The \textbf{k-means clustering} algorithm divides the clusters based on centroids. \textbf{Centroids} are imaginary or real locations representing the center of the cluster. The clusters are thus formed based on the distance of a node to a cluster centroid. In this case the number on nodes per cluster could be substantially different.

\section{Image Deblurring}
Here we used the Conjugate Gradient in order to deblur an image given the exact blurred image and the original transformation matrix. \\ \\
Matrix $B$ represents the blurred image, vector $b$ is the vectorized form of matrix $B$, matrix $X$ represents the original grayscale image -- where every entry corresponds to a pixel, the vector $x$ is the vectorized form of matrix $X$, and matrix $A$ indicates the transformation matrix coming from the repeated application of the \textbf{image kernel} -- which is the bulr effect in this case. The sizes of the presented matrices and vectors are:

\begin{itemize}
	\item $A \rightarrow n^2 \times n^2$
	\item $B \rightarrow n^2 \times n^2$
	\item $X \rightarrow n \times n$
	\item $\overrightarrow{b} \rightarrow n^2$
	\item $\overrightarrow{x} \rightarrow n^2$
\end{itemize}
With all of these matrices and vectors defined, we can write the following system of equations:

\[ Ax = b \] \\
By solving this system, we can recover the original image, getting rid of the blur effect. The blurred effect in the image is given by a weighted average of the surrounding pixels to a pixel. These weights are defined by the kernel matrix $K$. From this matrix, we can obtain the matrix $A$ such that the non-zero elements of each row of $A$ correspond to the values of $K$. \\ \\
$A$ is a $d^2$ banded matrix -- where $d \ll n$. This means that $A$ is a sparse matrix.

\subsection{Conjugate Gradient Method}
\subsubsection{Motivation}
The complexity of Gaussian Elimination for very large linear systems of equations is too high. For this reason we need to find other ways to solve these linear systems of equations. There are two types of methods that we can use:

\begin{itemize}
	\item \textbf{Direct Methods}
	\vspace{.2cm} \\
	Used to compute the exact solution in $n$ steps. This method has a very high memory consumption due to additional fill-ins.
	
	\item \textbf{Iterative Methods}
	\vspace{.2cm} \\
	This methods use an arbitrary initial starting point in order to compute an approximate arbitrary solution. For this method there is no need for additional memory and it will converge after a few iterations. On the other side, these methods are very often less robust and not as general as direct methods.
\end{itemize}

\subsubsection{Definitions}
The \textbf{error} in step $m$ is the deviation of the computed point from the exact solution. This can be written as:

\[ e^{(m)} = x-x^{(m)} \] \\
Where $x^{(m)}$ is the computed point and $x$ is the exact solution. This error is not known during the iterations -- otherwise we would know the solution. \\ \\
The \textbf{residual} provides us with a measure of the real error. This is computed as:

\[ r^{(m)} = b-Ax^{(m)} \] \\
Where $x^{(m)}$ is the computed point.

\subsubsection{Steepest Descent}
The \textbf{steepest descent} algorithm is a precursor to the Conjugate Gradient algorithm. Here we do the following operations:

\begin{enumerate}
	\item Start with a random initial guess
	\item Take the gradient -- which is the direction of the deepest descent
	\item Compute the minimum of the gradient
	\item Take the new gradient s.t. it is orthogonal to the previous gradient
	\item Repeat steps 2 to 4 until a certain convergence criterion is met
\end{enumerate}
This algorithm as a very low convergence rate, but it does not take optimal steps in order to find the next approximate solution.

\subsubsection{Conjugate Gradient}
The speed of convergence of the Conjugate Gradient is determined by the condition number $\kappa(A)$ of matrix $A$. The larger $\kappa(A)$, the slower the improvement. \\ \\
If matrix $A$ were not to be positive-definite, we can apply this trick in order to obtain a positive-definite matrix back:

\[ A^TAx = A^Tb \rightarrow \tilde{A}x = \tilde{b} \] \\
The premultiplication of $A$ with $A^T$ will result in the positive-definite matrix $\tilde{A}$. \\ \\
In order to carry out the actual method, we can use a slightly modified version of the steepest descent. The difference between the two methods is that in the case of the Conjugate Method, the two gradients need to be $A$-orthogonal. This means that:

\[ d^T_i A d_j = 0 \] \\
Where $d_i$ and $d_j$ are two vectors. By doing so, this would result in the new residual being orthogonal to the old residual. Furthermore, it would mean that every new step will never redo or undo optimizations of previous steps.

\subsection{Condition Number}
If a small modification in $b$ results in a big change in $x$, then the system is said to be \textbf{ill-conditioned}, and the condition number $\kappa(A)$ will be big. This condition number can be computed as:

\[ \kappa(A) = \frac{\sigma_{\max}}{\sigma_{\min}} \] \\
Where $\sigma$ indicates the singular values of $A$. For a real symmetric matrix, the singular values are equal to the absolute value of the eigenvalues.

\[ \sigma = |\lambda| \]

\subsection{Preconditioner}
A symmetric positive preconditioner $P$ is selected such that $P^{-1}$ approximates to $\tilde{A}^{-1}$.
\begin{align*}
	P^{-1}\tilde{A}x & = P^{-1}\tilde{b} \\
	(L^{-1}\tilde{A}L^{-1})(Lx) & = L^{-1}\tilde{b} \\
	\bar{\tilde{A}}\bar{x} & = \bar{\tilde{b}}
\end{align*}
Where

\[ \kappa(M^{-1}\tilde{A}) \ll \kappa(\tilde{A}) \]\\
This is done in order to both decrease the condition number and to decrease the range of the eigenvalues.
The computation of the preconditioner should be inexpensive to find. This is why we use the \textbf{Incomplete Cholesky factorization} in order to compute the preconditioner. By using this method, we only compute the Cholesky factors for the non-zero elements of $\tilde{A}$. This will give us the preconditioner:

\[ P = F^TF \] \\
Where $F$ is composed by the sparse IC factors. This routine can fail since the existence of $F$ is not guaranteed. In order to amend to this issue, we can apply a diagonal shift to $P$. By doing so, we enforce positive-definiteness, thus making $F$ computable.

\section{Linear Programming}
\textbf{Linear programming} is an optimization method which aims to maximize or minimize a linear objective function subject to linear equality or inequality constraints. Such an example is:

\begin{align*}
	\max~~ & \sum_{i = 1}^n c_ix_i \\
	\text{s.t.}~~ & \sum_{j = 1}^n a_{1,j}x_j \leq h_1 \\
	~~&~~ \vdots \\
	~~& \sum_{j = 1}^n a_{m,j}x_j \leq h_m
\end{align*}
In this case we have the presence of linear constrains. This means that the values of the vector $x_i$ cannot assume any arbitrary value. The following needs to be respected:

\begin{itemize}
	\item Satisfy a set of relationships
	\item Satisfy the non-negativity condition
	\begin{itemize}
		\item Only positive values are accepted
		\item Values need to belong to the feasible region
	\end{itemize}
\end{itemize}
The previous linear problem can be re-written by using the following simplified notation:

\begin{equation*}
  \begin{split}
    \max~~ & z = C^Tx \\
	\text{s.t.}~~ & Ax \leq h \\
	~~& x \geq 0
  \end{split}
\quad\quad\quad\quad
  \begin{split}
    \min~~ & z = C^Tx \\
	\text{s.t.}~~ & Ax \geq h \\
	~~& x \geq 0
  \end{split}
\end{equation*}
Where $z = f(x)$ represents the value of the objective function, $c$ is a vector of coefficients, $x$ is a vector of unknowns, $A$ is the coefficient matrix, and $h$ is the right hand side of the constraints. The sizes of these matrices are vectors are:

\begin{itemize}
	\item $A \rightarrow m \times n$
	\item $\overrightarrow{c} \rightarrow n$
	\item $\overrightarrow{x} \rightarrow n$
	\item $\overrightarrow{h} \rightarrow m$
\end{itemize}
This is known as the \textbf{standard form} of a linear program. After writing the problem in a standard form, we need to graphically plot the feasible region which satisfies all of the constraints.

\subsection{Fundamental Theorem of Linear Programming}
In order to find the optimal value, we can make use of the \textbf{Fundamental Theorem of Linear Programming}.

\blockquote{
If a linear program admits a solution -- i.e. if its neither unfeasible nor unbouded, this solution will lie in a vertex of the polytope defined by the feasible region. In the case in which two vertices are both maximizers or minimizers of the objective function, then also all the points lying on the line segment between them will represent optimal solutions of the linear programming problem.}

\subsection{Simplex Method}
The \textbf{simplex method} is an algorithm used for solving linear programming problems. It has an exponential worst-case complexity. In order to solve a linear programming problem with the simplex method, we need to do the following:

\begin{itemize}
	\item Write the problem in standard form
	\item Transform inequalities into equalities by adding:
	\begin{itemize}
		\item \textbf{Slack Variables} -- for maximization
		\item \textbf{Surplus Variables} -- for minimization
	\end{itemize}
	\item Add the variables above also to the standard form -- with their coefficient equal to 0
\end{itemize}
The steps above will result in, for example:

\begin{align*}
	\max~~ & z = 3x+2y+0s_1+0s_2 \\
	\text{s.t.}~~ & x+2y+s_1 = 4 \\
	~~& x-y+s_2 = 1 \\
	~~& x,y \geq 0;~ s_1, s_2 \geq 0
\end{align*}
The existence of a feasible solution implies also the existence of an optimal basic solution. The existence of an optimal solution implies the existence of an optimal basic solution.

\subsection{Auxiliary Minimization Problem}
By solving an initial \textbf{auxiliary minimization problem}, we find a feasible starting solution. It can be defined as follows -- by introducing artificial variables $u_1, ..., u_m$:

\begin{align*}
	\max~~ & \sum_{j = 1}^n u_i \\
	\text{s.t.}~~ & \sum_{j = 1}^n a_{1,j}x_j+s_1+u_1 = h_1 \\
	~~&~~ \vdots \\
	~~& \sum_{j = 1}^n a_{m,j}x_j+s_m+u_m = h_m \\
	~~& x_i, ..., x_n \geq 0;~ s_1, ..., s_m \geq 0;~ u_1, ..., u_m \geq 0
\end{align*}
The auxiliary problem aims at minimizing the sum of the artificial variables. The optimal solution of the auxiliary problem would be achieved when all of the artificial variables have a value of 0. \\ \\

\subsection{Stopping Criterion}
At every operation of the simplex method, we need to check if a \textbf{stopping criterion} is met. This criterion is:

\[ r_D = c_D - c_BB^{-1}D \] \\
Where $c_D$ represents the non-basic coefficients of the objective function, $c_B$ represents the basic coefficient of the objective function, $B^{-1}$ represents the inverse of the basic matrix, and $D$ represents the non-basic matrix. The optimality condition is given by $r_D \geq 0$ for minimization problems, and $r_D \leq 0$ for maximization problems. \\ \\

\subsection{Iterative Rule}
If the stopping criterion is not met, we select the highest reduced cost coefficient (in the case of maximization), or the lowest reduced cost coefficient (in the case of minimization). In case of multiple variables having the same values, we could end up swapping the same two variables over and over. In order to remove this problem, we use the \textbf{iterative rule}. \\ \\
The case described above can be fixed by computing the following ratio for every variable:

\[ \frac{B^{-1}h}{B^{-1}D} \] \\
Where $B^{-1}$ represents the inverse of the basic matrix, $h$ represents the right hand side of the constraints, and $D$ represents the non-basic matrix. Among all of the obtained ratios, we need to select the one that has the smallest positive value.

\end{document}






























 
