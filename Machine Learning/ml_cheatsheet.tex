% Copyright 2021 Edoardo Riggio

% Licensed under the Apache License, Version 2.0 (the "License");
% you may not use this file except in compliance with the License.
% You may obtain a copy of the License at

% 	http://www.apache.org/licenses/LICENSE-2.0

% Unless required by applicable law or agreed to in writing, software
% distributed under the License is distributed on an "AS IS" BASIS,
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
% See the License for the specific language governing permissions and
% limitations under the License.

\documentclass{article}

\usepackage{hyperref, amsmath, graphicx, amssymb}
\usepackage{fancyvrb, newverbs, xcolor, tikz}
\usepackage[latin1]{inputenc}

\usetikzlibrary{positioning}

\graphicspath{{./assets/}}
\definecolor{cverbbg}{gray}{0.93}

\newenvironment{cverbatim}
 {\SaveVerbatim{cverb}}
 {\endSaveVerbatim
  \flushleft\fboxrule=0pt\fboxsep=.5em
  \colorbox{cverbbg}{\BUseVerbatim{cverb}}%
  \endflushleft
}

\newenvironment{lcverbatim}
 {\SaveVerbatim{cverb}}
 {\endSaveVerbatim
  \flushleft\fboxrule=0pt\fboxsep=.5em
  \colorbox{cverbbg}{%
    \makebox[\dimexpr\linewidth-2\fboxsep][l]{\BUseVerbatim{cverb}}%
  }
  \endflushleft
}

\newcommand{\ctexttt}[1]{\colorbox{cverbbg}{\texttt{#1}}}
\newverbcommand{\cverb}
  {\setbox\verbbox\hbox\bgroup}
  {\egroup\colorbox{cverbbg}{\box\verbbox}}
  
\tikzset{%
  every neuron/.style={
    circle,
    draw,
    minimum size=1cm
  },
  neuron missing/.style={
    draw=none, 
    scale=4,
    text height=0.333cm,
    execute at begin node=\color{black}$\vdots$
  },
}

\begin{document}
\begin{titlepage}
    \begin{center}
        \vspace*{1cm}
        
        \Huge
        \textbf{Machine Learning Cheatsheet}
        
        \vspace{0.5cm}
        \LARGE
        
        \vspace{.5cm}
        
        Edoardo Riggio
   		  \vspace{1.5cm}
       
        \vfill
        
        \today
        
        \vspace{.8cm}
          \Large
          Machine Learning - S.P. 2022 \\
        Computer Science\\
        Universit\`{a} della Svizzera Italiana, Lugano\\
        
    \end{center}
\end{titlepage}

\tableofcontents

\newpage

\section{Introduction}
What does it mean "to learn"? We have to different definitions, one from a \textbf{statistical perspective}, and one from a \textbf{computer science perspective}.

\begin{itemize}
	\item \textbf{Statistical Perspective}
	\vspace{.2cm} \\
	Vast amounts of data are being generated in many fields. The statistician's job is to make sense of all of this data, extract meaningful patterns and trends, and understand "what the data says". This approach is also known as \textbf{learning from data}.
	
	\item \textbf{Computer Science Perspective}
	\vspace{.2cm} \\
	The field of machine learning is concerned with how to construct computer programs that automatically improve with experience.
\end{itemize}

\subsection{Mitchell's Formalisation}
A computer program is said to learn from \textbf{experience $E$} -- concerning some class of \textbf{task $T$}, and \textbf{performance measurement $P$} -- if its performance at task $T$, as measured by $P$, improves with experience $E$.

\subsection{Models}
\subsubsection{White Box Model}
In this case, both physical laws and structural parameters of the problem are known. A family equation can be derived.

\subsubsection{Grey Box Model}
The physical laws are known in this case, and at least one parameter is unknown. A family of equations can be derived, but the parameters need to be identified.

\subsubsection{Black Box Model}
In this case, the physical laws are unknown. A family of equations cannot be derived.

\subsection{Measures and Measurements}
The operation of measuring an unknown quantity $x_0$ can be modeled as taking an instance -- i.e., a \textbf{measurement} -- $x_i$ at time $i$, with an ad-hoc sensor $S$. \\ \\
Although $S$ has been suitably designed and realized, the physical elements that compose it are far from ideal and introduce uncertainties in the measurement process. As a result, $x_i$ only represents an estimate of $x_0$.

\subsection{Types of Models}
\subsubsection{Additive Model}
The measurement process can be modeled as:
\[ x = x_0 + \eta ~~~~~\text{where}~\eta = f_n(0, \sigma^2_\eta) \]
Where $\eta$ is an independent and identically distributed random variable, the model assumes that the i.i.d. noise does not depend on the working point $x_0$.

\subsection{Multiplicative Model}
The measurement process can be modeled as:
\[ x = x_0 (1 + \eta) ~~~~~\text{where}~\eta = f_n(0, \sigma^2_\eta) \]
Where $\eta$ is an independent and identically distributed random variable, the noise, in this case, depends on the working point $x_0$. In absolute terms, the impact of the noise on the signal is $x_0 \eta$, but the relative contribution is $\eta$ -- which does not depend on $x_0$.

\subsection{Supervised Learning}
In a supervised learning framework, we have the following elements: a \textbf{concept to learn}, a \textbf{teacher}, and a \textbf{student}.

\subsubsection{Regression}
The goal of regression is to determine the function that explains the given instances -- \textbf{measuremets}. The student proposes a family of models $f(\theta, x)$, and after a learning procedure, the "best" model $f(\hat \theta, x)$ is found.

\subsubsection{Classification}
The goal of classification is to determine the function -- \textbf{model} -- that partitions the input -- \textbf{measurements} -- into classes. The student proposes a family of models $f(\theta, x)$, and after a learning procedure, the "best" model $f(\hat \theta, x)$ is found.

\subsubsection{Prediction}
The goal of prediction is to tell us which data -- \textbf{measurements} -- will come next, possibly along with a confidence level. The student proposes a family of models $f(\theta, x)$, and after a learning procedure, the "best" model $f(\hat \theta, x)$ is found.

\subsection{Features}
We might want to extract features from the measurements to ease the learning task. The features must:
\begin{itemize}
	\item Provide a compact representation of inputs
	\item Be particularly advantageous if we have prior information to take advantage of
	\item Be reduced to a minimal set before processing them for task solving
\end{itemize}

\subsection{Unsupervised Learning}
The goal of unsupervised learning is to build a representation of data. During its operational life, given an input, the machine provides information that can be used for decision-making.

\section{Linear Regression}
To prepare a linear regression model, several techniques can be used. The most popular being the \textbf{Least Mean Square (LMS)}, and the \textbf{Gradient Descent}. \\ \\
Linear models are good when we have the following conditions:
\begin{itemize}
	\item The data is generated by a linear model
	\item The dataset is small
	\item The data is sparse
	\item The uncertainty is high
\end{itemize}

\subsection{Muliple Linear Regression}
We are given a set of points:
\[ \{ (x_1, y_1), (x_2, y_2), \dots, (x_n, y_n) \} ~~~~~ \text{where}~x \in \mathbb{R}^d, y \in 	\mathbb{R} \]
This is known as the \textbf{training set}. We now assume that the unknown function that generates the data is linear. Moreover, we assume that there is a gaussian uncertainty affecting the measurements. The model is given as:
\[ y(x) = \theta^0_1 + \theta^0_2 z_2 + \dots + \theta^0_d z_d ~~~~ \text{where}~\theta^0 \in \mathbb{R}, \eta = N(0, \sigma^2_\eta) \]
Which can be written in its canonical form:
\[ y(x) = x^T \theta^0 + \eta ~~~~~ \text{where}~x^T = \begin{bmatrix} 1 & z_2 & \cdots & z_d \end{bmatrix}, \theta^{0^T} = \begin{bmatrix} \theta^0_1 & \theta^0_2 & \cdots & \theta^0_d \end{bmatrix} \]
Both the optimal parameters and the variance of the noise are unknown. Since we know that the system model is linear, the family of models which best fits the data is:
\[ \hat y(x) = f(\theta, x) = x^T \theta \]
In order to determine the best parameters, we estimate them:
\[ f(\hat \theta, x) = x^T \hat \theta \]

\subsubsection{Least Mean Square}
The main idea of this procedure is to select the linear function that minimizes the average distance between the given points and the linear function. \\ \\
The \textbf{performance function} of this method is the following:
\[ V_n(\theta) = \frac{1}{n} \sum^n_{i=1}(y(x_i) - f(\theta, x_i))^2 \]
Therefore, the parameter vector $\hat \theta$ minimizing the performance function is the following:
\[ \hat \theta  = \arg\min_{\theta \in \Theta} V_n(\theta) \]

\subsubsection{Parameter Estimation}
By grouping the data into vectors $X$ -- of all $x$ values -- and $Y$ -- of all $y$ values, we can rewrite the formula above in its canonical form:
\begin{align*}
	V_n^*(\theta) &= \sum^n_{i=1} (y(x_i) - x^T_i \theta)^2 \\
	&= (Y - X\theta)^T (Y - X\theta) 
\end{align*}
\textbf{Stationary points} are those for which:
\[ \frac{\partial V_n^*(\theta)}{\partial \theta} = -2X^T Y + 2X^T X \theta = 0 \]
Therefore, the parameter vector $\hat \theta$ minimizing the performance function is the following:
\[ \hat \theta = (X_TX)^{-1}X^TY \]
And the best approximating model is:
\[ f(\hat \theta x) = x^T\hat\theta \]

\subsection{Performance at Task}
In order to test the model, we need to use another set of unseen data. This is because the \textbf{performance at task}:
\[ V_n(\hat\theta) = \frac{1}{n} \sum^n_{i=1}(y(x_i) - f(\theta, x_i))^2 \]
It is biased to the training set. For this reason, we consider another set of data -- different from the training set -- and call it \textbf{test set}:
\[ \{ (\bar x_1, \bar y_1), (\bar x_2, \bar y_2), \dots, (\bar x_l, \bar y_l) \} \]
And evaluate the performance at task on it:
\[ V_l(\hat\theta) = \frac{1}{l} \sum^l_{i=1}(\bar y_i - \bar x^T\hat\theta)^2 \]

\subsection{Cross-Validation}
Cross-validation provides a means to assess the performance of a model. This method can also be considered for model selection -- with some care. In the latter case, we consider the model that minimizes
\[ V_l(\hat\theta) = \frac{1}{l} \sum^l_{i=1}(\bar y_i - \bar x^T\hat\theta)^2 \]

\subsection{Properties}
Under the linear framework presented earlier, it can be proved that both implications hold true:
\[ \lim_{n \rightarrow \infty} \hat\theta = \theta^0 \]
\[ \lim_{l \rightarrow \infty} V_l(\hat\theta) = \sigma^2_\eta \]
In addition, assuming that $n$ training couples are given, then it can be proved that:
\[ Var(\hat\theta) = (X^TX)^{-1}\sigma^2_\eta ~~~~~ \text{with}~\hat\sigma^2_\eta = \frac{1}{n - d} \sum^n_{i = 1}(y(x_i) - f(\hat\theta, x_i))^2 \]
If a parameter is smaller than twice its standard deviation, it must be set to 0. After which, we re-evaluate the performance and decide whether to keep it. This is known as \textbf{Occam's razor strategy}.

\subsection{Ridge Regression}
Ridge regression aims at pushing as many parameters as possible towards zero. This is done by adding a shrinking penalty to the loss function. The \textbf{MSE} training performance mesure
\[ V_n(\theta) = \frac{1}{n} \sum^n_{i=1}(y_i - x_i^T\theta)^2 \]
Now becomes
\[ V_{Ridge}(\theta) = \frac{1}{n} \sum^n_{i=1}(y_i - x_i^T\theta)^2 + \lambda \sum^d_{i=2}\theta^2_i \]
Where $\lambda$ is a hyperparameter weighting the two contributions. A smaller $\lambda$ gives more importance to accuracy, while a high $\lambda$ priviledges a smaller number of parameters in the model. A tradeoff can be obtained by estimating and appropriate $\lambda$ thanks to the \textbf{validation set}.

\subsection{Lasso Regression}
Lasso regression also aims at pushing as many parameters as possible towards zero. In this case, however, we penalize the parameter itself rather than its square. The \textbf{MSE} now is:
\[ V_{Lasso}(\theta) = \frac{1}{n} \sum^n_{i=1}(y_i - x_i^T\theta)^2 + \lambda \sum^d_{i=2}|\theta_i| \]
The optimization problem is not convex anymore, and the loss function is not differentiable. This makes the problem computationally complex and solvable only via optimization techniques such as quadratic programming.
 
\subsection{Final Prediction Error}
The expected prediction error is computed as follows:
\[ FPE = \frac{n+d}{n-d}\sum^n_{i=1}(y(x_i) - f(\hat\theta, x_i))^2 \]
This measurement can be used for both model selection -- only on hierarchical family of models, and for assessing the performance of unseen data.
 
\section{Non-Linear Regression}
To determine the stationary points of a differentiable function we use the following minimization problem:
\[ \hat\theta = \arg \min_{\theta \in \Theta} V_n(\theta) \]
This minimization is carried out by a \textbf{gradient descent-based procedure}.
 
\subsection{Gradient-Based Optimization}
Given a generic convex and differentiable scalar function $y = f(x)$, we do the following to determine the unique stationary point. \\ \\
We start from an initial point $x(0) = x_0$ and start moving along the gradient in the direction minimizing the loss function. To do so we use the following formula:
\[ \theta_{i+1} = \theta_i - \varepsilon_L \frac{\partial V_n(\theta)}{\partial \theta}|_{\theta_i} \]
The minimization procedure applied to a non-linear function -- and based on a training set -- is also known as \textbf{learning procedure}. \\ \\
When minimizing, we might encounter what is known as the \textbf{identifiability problem}. This happens when the function to be optimized is not convex, and there are several local or global minima in the function.

\subsection{Structural Risk}
The structural risk of the estimated model is its generalization ability. To compute the function's generalization ability, we use the following formula:
\[ \bar V(\hat\theta) = \int L(y, f(\hat\theta, x)) p_{x, y} dxy \]
Where $L$ is the \textbf{loss function}. The risk associated with the model's generalization ability can be decomposed into three terms:
\[ \bar V(\hat\theta) = (\bar V(\hat\theta) - \bar V(\theta^0)) + (\bar V(\theta^0) - V_I) + V_I \]

\subsubsection{Inherent Risk}
The inherent risk depends entirely on the structure of the learning problem. This term can be improved only by improving the problem itself, such as reducing the noise caused by the measurement instruments. \\ \\
The inherent risk is computed as:
\[ V_I \]

\subsubsection{Approximation Risk}
The approximation risk depends on how close the approximating family of models is to the function generating the data ($g(x)$). We can use a more appropriate family of models to reduce this risk. \\ \\
The approximation risk is computed as:
\[ \bar V(\theta^0) - V_I \]

\subsubsection{Estimation Risk}
The estimation risk depends on the effectiveness of the learning procedure. We can use a more effective learning procedure to reduce this risk. \\ \\
The estimation risk is computed as:
\[ \bar V(\hat\theta) - \bar V(\theta^0) \]

\section{Feedforward Neural Networks}
A neural network has the following structure \\ \\

\begin{tikzpicture}[x=1.5cm, y=1.5cm, >=stealth]
\foreach \m/\l [count=\y] in {1,2,missing,3}
  \node [every neuron/.try, neuron \m/.try] (input-\m) at (0,1.5-\y) {};

\foreach \m [count=\y] in {1,2,3,4,missing,5}
  \node [every neuron/.try, neuron \m/.try ] (hidden-\m) at (2,2.5-\y) {};

\node [every neuron/.try, neuron 1/.try ] (output-1) at (4,1-2) {};
\foreach \l [count=\i] in {1,2,n}
  \draw [<-] (input-\i) -- ++(-1,0)
    node [above, midway] {$x_\l$};

\draw [->] (output-1) -- ++(1,0)
  node [above, midway] {$f(\theta, x)$};

\foreach \i in {1,...,3}
  \foreach \j in {1,...,5}
    \draw [->] (input-\i) -- (hidden-\j);

\foreach \i in {1,...,5}
  \draw [->] (hidden-\i) -- (output-1);

\foreach \l [count=\x from 0] in {Input, Hidden, Ouput}
  \node [align=center, above] at (\x*2,2) {\l \\ layer};
\end{tikzpicture} \\ \\
Each connection between neurons has a weight of $w_n$. Moreover, the overall weight of a sigle neuron is computed as:
\[ a_t = \sum^n_{i=1}x^i w^i \]
Each neuron in the hidden layer has an activation function. This activation function can be of several different types:

\begin{itemize}
	\item \textbf{Sigmoidal}
	\vspace{.2cm} \\
	This activation function is mainly used by the neurons of the hidden layer. The sigmoidal formula is:
	\[ Sig(x) = \frac{1}{1 + e*{-x}} \]
	
	\item \textbf{Hyperbolic Tangent}
	\vspace{.2cm} \\
	This activation function is an alternative to the sigmoidal activation function. This function is useful when we have back-propagation. The hyperbolic tangent formula is:
	\[ HT(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}} \]
	
	\item \textbf{Linear}
	\vspace{.2cm} \\
	The neurons of the output layer mainly use this activation function.
\end{itemize}

\subsection{Universal Approximation Theorem}
A feedforward neural network with a single hidden layer containing a finite number of neurons and a linear output neuron approximates any continuous function defined on compact subsets.

\subsection{Early Stopping}
The idea of early stopping is to start from an over-dimensioned network and try to find where the models start diverging.

\begin{center}
	\includegraphics[width=10cm]{early_stopping.png}
\end{center}
To evaluate the model, we use another set of data called \textbf{validation set}. We cannot use the test set for the model selection during training. Otherwise, the model performance assessment would be biased.

\subsection{Splitting the Data}
To split the data we have -- which has length $n$, we apply a ratio (which is application-dependent). This ratio is computed in the function of:

\begin{itemize}
	\item The size of $n$
	\item The complexity of the model
\end{itemize}
For example:
\[ n = 1,000 \rightarrow Tr = 70\%,~ V = 15\%,~ Te = 15\% \]
\[ n = 1,000,000 \rightarrow Tr = 99\%,~ V = 0.5\%,~ Te = 0.5\% \]

\section{Classification Problem}
Given an input vector $x$, and a set of classes $C_1, C_2, \dots, C_k$, we wish to assign $x$ to the most appropriate class. In classification, $y$ assumes a categorical value.

\subsection{Binary Classifier}
Given an input vector $x$ of two features and two classes, we divide the features in a binary manner -- i.e., either it is in one group or the other.

\begin{center}
	\includegraphics[width=6cm]{binary_classification.png}
\end{center}
Given a training set, we obtain the following estimated parameter:
\[ \hat\theta = (X^TX)^{-1}X^TX \]
And given a new $x$, we evaluate the following discriminant function:
\[ f(\hat\theta, x) = x^T\hat\theta \]
If the discriminant value is above $0.5$, then $x$ will be part of one group. Otherwise, $x$ will be part of the other group.

\subsection{Bayes Classifier}
The Bayes classifier deals with multi-class problems. Here we assign a probability to each class and select the one that maximizes such probability.
\[ Pr(Y = k|X = x) \]
Where $X$ and $Y$ are two random variables. The Bayes theorem described above provides us with a way to express the posterior conditional probability as:
\[ Pr(Y = k|X = x) = \frac{Pr(X = x|Y = k) \cdot Pr(Y = k)}{Pr(X = x)} \]
Where $Pr(Y = k|X = x)$ is the \textbf{posterior probability}, $Pr(X = x|Y = k)$ is the \textbf{likelihood}, $Pr(Y = k)$ is the \textbf{prior probability}, and $Pr(X = x)$ is the evidence.

\begin{center}
	\includegraphics[width=6cm]{bayes_classification.png}
\end{center}
This classifier is said to be na\"ive. It is because it makes the strong assumption of independence among the features. The Bayes classifier is optimal if the assumptions were met and the probabilities are known.

\subsection{K-Nearest Neighbour Classifier}
This type of classifier looks at the $k$-nearest neighbors of point $x$ to make the classification decision. For example, the Euclidean distance can be used to determine the class.

\begin{center}
	\includegraphics[width=6cm]{nearest_neighbors.png}
\end{center}
In this case, no training phase is required, but we have a high computational cost in evaluating the distances.

\end{document}

































